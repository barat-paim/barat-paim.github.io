<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on barath.xyz</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on barath.xyz</description>
    <generator>Hugo -- 0.136.2</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Oct 2024 12:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>finetuning Llama3.2 1B</title>
      <link>http://localhost:1313/posts/firstpost/</link>
      <pubDate>Fri, 18 Oct 2024 12:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/firstpost/</guid>
      <description>&lt;h1 id=&#34;status-update-log&#34;&gt;Status Update Log&lt;/h1&gt;
&lt;h2 id=&#34;v001---initial-setup-and-project-kickoff&#34;&gt;v0.0.1 - Initial Setup and Project Kickoff&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Created the project directory structure and initialized version control.&lt;/li&gt;
&lt;li&gt;Set up the basic environment using Python 3.12 and installed the required libraries for Transformer-based model training, including PyTorch and HuggingFace Transformers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v002---dataset-preparation&#34;&gt;v0.0.2 - Dataset Preparation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Selected the SQuAD dataset for fine-tuning the model.&lt;/li&gt;
&lt;li&gt;Loaded the dataset and performed initial data cleaning and formatting.&lt;/li&gt;
&lt;li&gt;Created scripts for dataset exploration to understand question-answer distribution and context.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v003---model-selection&#34;&gt;v0.0.3 - Model Selection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Chose the LLaMA-3.2-1B model as the base for fine-tuning due to its capabilities with language modeling tasks.&lt;/li&gt;
&lt;li&gt;Downloaded and set up the pre-trained model in the development environment.&lt;/li&gt;
&lt;li&gt;Verified model loading and initial inference to ensure correct setup.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v004---experimentation-with-quantization&#34;&gt;v0.0.4 - Experimentation with Quantization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tested quantization techniques to fit the LLaMA model in available GPU memory.&lt;/li&gt;
&lt;li&gt;Implemented 8-bit quantization using &lt;code&gt;BitsAndBytesConfig&lt;/code&gt; to reduce memory footprint.&lt;/li&gt;
&lt;li&gt;Quantization helped in reducing memory usage and allowed the model to run smoothly on the g4dn.xlarge instance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v005---gradient-checkpointing-and-mixed-precision&#34;&gt;v0.0.5 - Gradient Checkpointing and Mixed Precision&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Enabled gradient checkpointing to save memory during backpropagation by recalculating activations on-the-fly.&lt;/li&gt;
&lt;li&gt;Utilized mixed precision training (fp16) to further optimize memory usage.&lt;/li&gt;
&lt;li&gt;Both approaches combined allowed training without running into memory limitations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v006---parameter-efficient-fine-tuning-peft-setup&#34;&gt;v0.0.6 - Parameter-Efficient Fine-Tuning (PEFT) Setup&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implemented PEFT with LoRA to add trainable adapters to the LLaMA model.&lt;/li&gt;
&lt;li&gt;Used the &lt;code&gt;peft&lt;/code&gt; library for seamless integration of LoRA adapters.&lt;/li&gt;
&lt;li&gt;Ensured that the adapters effectively reduced the number of trainable parameters while maintaining model performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v007---custom-gpu-memory-allocation-checks&#34;&gt;v0.0.7 - Custom GPU Memory Allocation Checks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Encountered discrepancies between reported and actual available memory on the GPU instance.&lt;/li&gt;
&lt;li&gt;Wrote custom scripts to monitor GPU memory allocation and identify bottlenecks.&lt;/li&gt;
&lt;li&gt;Adjusted model configurations based on insights to ensure efficient use of GPU resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v008---optimizer-and-training-configuration&#34;&gt;v0.0.8 - Optimizer and Training Configuration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implemented &lt;code&gt;paged_adamw_8bit&lt;/code&gt; optimizer to offload optimizer states to CPU, reducing GPU memory requirements.&lt;/li&gt;
&lt;li&gt;Introduced gradient accumulation to simulate larger batch sizes without increasing memory usage.&lt;/li&gt;
&lt;li&gt;Configured learning rate schedules and optimizer settings to suit the training requirements of LLaMA-3.2-1B.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v009---initial-training-attempt-and-cuda-out-of-memory-oom-errors&#34;&gt;v0.0.9 - Initial Training Attempt and CUDA Out of Memory (OOM) Errors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First training attempt resulted in CUDA OOM errors.&lt;/li&gt;
&lt;li&gt;Identified causes related to batch size and learning rate settings.&lt;/li&gt;
&lt;li&gt;Made adjustments to the batch size and added a configuration to handle memory fragmentation (&lt;code&gt;PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v010---preventing-system-hibernation&#34;&gt;v0.1.0 - Preventing System Hibernation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Prevented the system from entering sleep, suspend, or hibernation states to avoid interruptions during training.
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Added symlinks for &lt;code&gt;/etc/systemd/system/sleep.target&lt;/code&gt;, &lt;code&gt;/suspend.target&lt;/code&gt;, &lt;code&gt;/hibernate.target&lt;/code&gt;, and &lt;code&gt;/hybrid-sleep.target&lt;/code&gt; to &lt;code&gt;/dev/null&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v011---iteration-speed-improvements&#34;&gt;v0.1.1 - Iteration Speed Improvements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Increased training iteration speed from 10s/iteration to 15s/iteration.&lt;/li&gt;
&lt;li&gt;Adjusted gradient checkpointing settings to reduce computation overhead.&lt;/li&gt;
&lt;li&gt;Achieved more stable training runs by reducing memory consumption through optimized batch sizes and accumulation steps.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v012---learning-rate-finder-and-checkpoint-issues&#34;&gt;v0.1.2 - Learning Rate Finder and Checkpoint Issues&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Initiated the learning rate search process to find an optimal rate for model convergence.&lt;/li&gt;
&lt;li&gt;Received warnings about incompatible settings (&lt;code&gt;use_cache=True&lt;/code&gt;) with gradient checkpointing; resolved by switching to &lt;code&gt;use_cache=False&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Iteration speed during learning rate search was approximately 5s/iteration, which needed adjustments to reduce overall time.&lt;/li&gt;
&lt;li&gt;Ended with a &lt;code&gt;KeyboardInterrupt&lt;/code&gt; after the 27th iteration, requiring further refinements in checkpointing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v013---learning-rate-finder-success&#34;&gt;v0.1.3 - Learning Rate Finder Success&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Successfully ran the learning rate finder without OOM errors.&lt;/li&gt;
&lt;li&gt;Estimated total time for the learning rate search was about 17 minutes, an acceptable duration.&lt;/li&gt;
&lt;li&gt;Achieved a significant improvement in stability, with consistent training progress without memory-related interruptions.&lt;/li&gt;
&lt;li&gt;Prepared for the main training loop using the identified learning rate.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v014---taming-the-llama-training-begins&#34;&gt;v0.1.4 - Taming the Llama: Training Begins&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Began fine-tuning LLaMA-3.2-1B on the SQuAD dataset.&lt;/li&gt;
&lt;li&gt;Set batch size to 12 and gradient accumulation steps to 3 for balanced training and memory efficiency.&lt;/li&gt;
&lt;li&gt;DataLoader workers were set to 4, contributing to efficient data handling during training.&lt;/li&gt;
&lt;li&gt;Iteration speed improved to 16.17s/iteration, allowing a full training cycle to complete in around 4 hours.&lt;/li&gt;
&lt;li&gt;Addressed frequent tokenizer parallelism warnings by setting the following environment variable:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;environ[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;TOKENIZERS_PARALLELISM&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;false&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v015---training-results-and-warnings-addressed&#34;&gt;v0.1.5 - Training Results and Warnings Addressed&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fine-tuned the LLaMA model over 834 iterations, successfully completing training.&lt;/li&gt;
&lt;li&gt;Observed loss reduction from &lt;strong&gt;9.73&lt;/strong&gt; initially to &lt;strong&gt;7.95&lt;/strong&gt; at the end of epoch 3.&lt;/li&gt;
&lt;li&gt;Learning rate started from &lt;strong&gt;0.0099&lt;/strong&gt; and adjusted dynamically during training for optimal learning.&lt;/li&gt;
&lt;li&gt;Validation losses reduced consistently, reaching &lt;strong&gt;8.11&lt;/strong&gt; during regular evaluations.&lt;/li&gt;
&lt;li&gt;Encountered warnings related to quantization and checkpointing, resolved by modifying training parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v016---evaluation-planning-for-llama-model&#34;&gt;v0.1.6 - Evaluation Planning for LLaMA Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Developed an evaluation plan to assess the performance of the fine-tuned LLaMA-3.2-1B model on the SQuAD dataset.&lt;/li&gt;
&lt;li&gt;Key steps included:
&lt;ol&gt;
&lt;li&gt;Loading the fine-tuned model and tokenizer using the Transformers library.&lt;/li&gt;
&lt;li&gt;Defining evaluation metrics, focusing on Exact Match (EM) and F1 Score.&lt;/li&gt;
&lt;li&gt;Preparing the validation set from the SQuAD dataset, loaded through DataLoader.&lt;/li&gt;
&lt;li&gt;Generating answers using the fine-tuned model to predict responses for questions in the validation set.&lt;/li&gt;
&lt;li&gt;Calculating EM and F1 scores for each prediction to evaluate performance.&lt;/li&gt;
&lt;li&gt;Looping through the entire validation set to derive average scores, providing insights into overall model efficacy.&lt;/li&gt;
&lt;li&gt;Saving evaluation results for future reference.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Next steps involve running the evaluation process, interpreting results, and identifying areas for further improvement or fine-tuning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v017---prevented-system-hibernation-note-added&#34;&gt;v0.1.7 - Prevented System Hibernation (Note Added)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Added a note regarding the persistence of power-saving feature changes across reboots:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo systemctl unmask sleep.target suspend.target hibernate.target hybrid-sleep.target
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v018---increased-training-iteration-speed-adjusting-for-duration&#34;&gt;v0.1.8 - Increased Training Iteration Speed, Adjusting for Duration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Increased iteration speed from 10s/iteration to 15s/iteration.&lt;/li&gt;
&lt;li&gt;However, the number of iterations also increased from 5000 to 8000, resulting in an estimated training duration of 32 hours.&lt;/li&gt;
&lt;li&gt;Searching for new ways to decrease overall training time.&lt;/li&gt;
&lt;li&gt;Attached are screenshots of progress for two different versions (try1 and try2) for comparison.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v019---learning-rate-search-and-issues-with-checkpointing&#34;&gt;v0.1.9 - Learning Rate Search and Issues with Checkpointing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Began the process of finding the optimal learning rate for the LLaMA model fine-tuning.&lt;/li&gt;
&lt;li&gt;Successfully loaded the SQuAD dataset and prepared it for training.&lt;/li&gt;
&lt;li&gt;GPU Memory updates:
&lt;ul&gt;
&lt;li&gt;After loading the tokenizer: Allocated: 0.00 GB, Reserved: 0.00 GB.&lt;/li&gt;
&lt;li&gt;After model loading with quantization and adding LoRA configuration: Allocated: 2.03 GB, Reserved: 2.64 GB.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Set up gradient checkpointing and started the learning rate search.&lt;/li&gt;
&lt;li&gt;Result: 5s/iteration it is taking 2 hours to find the learning rate&lt;/li&gt;
&lt;li&gt;Issues encountered:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;use_cache=True&lt;/code&gt; found incompatible with gradient checkpointing, switched to &lt;code&gt;use_cache=False&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Received user warnings regarding deprecated parameters (&lt;code&gt;use_reentrant&lt;/code&gt; parameter) and quantization specifics.&lt;/li&gt;
&lt;li&gt;The learning rate search indicated increasing learning rates with corresponding loss values.&lt;/li&gt;
&lt;li&gt;Ended with a KeyboardInterrupt after the 27th iteration, requiring adjustments to proceed further.&lt;/li&gt;
&lt;li&gt;Planning to run a subset of the data to reduce the time to 5 odd minutes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v0110---learning-rate-finder-running-without-oom-error&#34;&gt;v0.1.10 - Learning Rate Finder Running Without OOM Error&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The learning rate finder is now running without encountering Out of Memory (OOM) errors, which is a significant improvement.&lt;/li&gt;
&lt;li&gt;Current iteration speed is approximately 2.49 seconds per iteration, with an estimated total time of 17 minutes to complete.&lt;/li&gt;
&lt;li&gt;Observations and potential next steps:
&lt;ul&gt;
&lt;li&gt;Total time of 17 minutes is longer than the initial 10-minute target but still reasonable.&lt;/li&gt;
&lt;li&gt;Potential adjustments to reduce time further:
&lt;ul&gt;
&lt;li&gt;Reduce &lt;code&gt;num_iterations&lt;/code&gt; from 50 to 40 or 30.&lt;/li&gt;
&lt;li&gt;Increase &lt;code&gt;batch_size&lt;/code&gt; if GPU memory allows.&lt;/li&gt;
&lt;li&gt;Decrease &lt;code&gt;accumulation_steps&lt;/code&gt; if needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Once the learning rate finder completes, the optimal learning rate will be used for the main training loop.&lt;/li&gt;
&lt;li&gt;Monitoring the following after learning rate completion:
&lt;ul&gt;
&lt;li&gt;Optimal learning rate found.&lt;/li&gt;
&lt;li&gt;Initial training loss.&lt;/li&gt;
&lt;li&gt;Training speed in the main loop.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v0111---increased-batch-size-and-improved-training-time&#34;&gt;v0.1.11 - Increased Batch Size and Improved Training Time&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Improved training iteration speed (7sec -&amp;gt; 10sec -&amp;gt; 15.5sec -&amp;gt; 16.17 sec/it) by adjusting several hyperparameters:
&lt;ul&gt;
&lt;li&gt;Batch size increased to 12.&lt;/li&gt;
&lt;li&gt;Gradient accumulation steps set to 3.&lt;/li&gt;
&lt;li&gt;DataLoader workers set to 4.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Achieved better GPU utilization without running into Out of Memory (OOM) errors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v0112---evaluation-plan-execution-for-llama-model&#34;&gt;v0.1.12 - Evaluation Plan Execution for LLaMA Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Initiated the evaluation process of the fine-tuned LLaMA-3.2-1B model on the SQuAD validation set.&lt;/li&gt;
&lt;li&gt;Key stages completed:
&lt;ol&gt;
&lt;li&gt;Loaded the fine-tuned model and tokenizer from the saved directory.&lt;/li&gt;
&lt;li&gt;Defined Exact Match (EM) and F1 Score as evaluation metrics.&lt;/li&gt;
&lt;li&gt;Prepared the validation set using DataLoader, ensuring efficient data batching.&lt;/li&gt;
&lt;li&gt;Generated predictions for the validation set questions using a question-context input format.&lt;/li&gt;
&lt;li&gt;Calculated EM and F1 scores for each prediction.&lt;/li&gt;
&lt;li&gt;Evaluated the entire validation set to derive average EM and F1 scores.&lt;/li&gt;
&lt;li&gt;Saved evaluation results to a JSON file for future analysis.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Next steps involve analyzing the results, interpreting the model&amp;rsquo;s performance against benchmarks, and making decisions on further training or adjustments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v0113---final-training-summary-and-future-plans&#34;&gt;v0.1.13 - Final Training Summary and Future Plans&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Completed training and evaluation of LLaMA-3.2-1B on the SQuAD dataset.&lt;/li&gt;
&lt;li&gt;Achieved a final loss of &lt;strong&gt;7.95&lt;/strong&gt; and a validation loss of &lt;strong&gt;8.11&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Recorded average F1 Score and Exact Match (EM) metrics from the evaluation process.&lt;/li&gt;
&lt;li&gt;Evaluation results indicated areas of strong performance and potential for further fine-tuning.&lt;/li&gt;
&lt;li&gt;Planned next steps:
&lt;ul&gt;
&lt;li&gt;Analyze specific instances of lower performance to understand failure cases.&lt;/li&gt;
&lt;li&gt;Adjust hyperparameters based on evaluation insights.&lt;/li&gt;
&lt;li&gt;Consider additional rounds of fine-tuning or model architecture adjustments if needed.&lt;/li&gt;
&lt;li&gt;Start preparing for model deployment if evaluation results meet project benchmarks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>

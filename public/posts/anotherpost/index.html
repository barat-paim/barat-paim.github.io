<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Control or Convenience? Lessons from Implementing Vision-Language Models | barath.xyz</title>
<meta name="keywords" content="">
<meta name="description" content="Date: October 16, 2024
Introduction: This blog captures the learning journey of implementing vision-language models, exploring trade-offs between convenience and control, and documenting the practical steps involved.
Caption: &ldquo;Control or Convenience? Lessons from Implementing Vision-Language Models&rdquo;
Key Takeaways:
Quick Wins for Learning:
Start with tangible projects to quickly gain experience with new models.
Image captioning with Llama 3.2 Vision Instruct is a great entry project to understand vision-language integration.
API vs. Local Model Deployment:">
<meta name="author" content="">
<link rel="canonical" href="https://barath.xyz/posts/anotherpost/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://barath.xyz/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://barath.xyz/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://barath.xyz/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://barath.xyz/apple-touch-icon.png">
<link rel="mask-icon" href="https://barath.xyz/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://barath.xyz/posts/anotherpost/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Control or Convenience? Lessons from Implementing Vision-Language Models" />
<meta property="og:description" content="Date: October 16, 2024
Introduction: This blog captures the learning journey of implementing vision-language models, exploring trade-offs between convenience and control, and documenting the practical steps involved.
Caption: &ldquo;Control or Convenience? Lessons from Implementing Vision-Language Models&rdquo;
Key Takeaways:
Quick Wins for Learning:
Start with tangible projects to quickly gain experience with new models.
Image captioning with Llama 3.2 Vision Instruct is a great entry project to understand vision-language integration.
API vs. Local Model Deployment:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://barath.xyz/posts/anotherpost/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-10-16T12:00:00+00:00" />
<meta property="article:modified_time" content="2024-10-16T12:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Control or Convenience? Lessons from Implementing Vision-Language Models"/>
<meta name="twitter:description" content="Date: October 16, 2024
Introduction: This blog captures the learning journey of implementing vision-language models, exploring trade-offs between convenience and control, and documenting the practical steps involved.
Caption: &ldquo;Control or Convenience? Lessons from Implementing Vision-Language Models&rdquo;
Key Takeaways:
Quick Wins for Learning:
Start with tangible projects to quickly gain experience with new models.
Image captioning with Llama 3.2 Vision Instruct is a great entry project to understand vision-language integration.
API vs. Local Model Deployment:"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://barath.xyz/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Control or Convenience? Lessons from Implementing Vision-Language Models",
      "item": "https://barath.xyz/posts/anotherpost/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Control or Convenience? Lessons from Implementing Vision-Language Models",
  "name": "Control or Convenience? Lessons from Implementing Vision-Language Models",
  "description": "Date: October 16, 2024\nIntroduction: This blog captures the learning journey of implementing vision-language models, exploring trade-offs between convenience and control, and documenting the practical steps involved.\nCaption: \u0026ldquo;Control or Convenience? Lessons from Implementing Vision-Language Models\u0026rdquo;\nKey Takeaways:\nQuick Wins for Learning:\nStart with tangible projects to quickly gain experience with new models.\nImage captioning with Llama 3.2 Vision Instruct is a great entry project to understand vision-language integration.\nAPI vs. Local Model Deployment:\n",
  "keywords": [
    
  ],
  "articleBody": "Date: October 16, 2024\nIntroduction: This blog captures the learning journey of implementing vision-language models, exploring trade-offs between convenience and control, and documenting the practical steps involved.\nCaption: “Control or Convenience? Lessons from Implementing Vision-Language Models”\nKey Takeaways:\nQuick Wins for Learning:\nStart with tangible projects to quickly gain experience with new models.\nImage captioning with Llama 3.2 Vision Instruct is a great entry project to understand vision-language integration.\nAPI vs. Local Model Deployment:\nAPI Approach: Easy, fast setup with minimal control; ideal for prototyping.\nLocal Model Deployment: Requires infrastructure management but offers full control and cost efficiency for larger workloads.\nControl vs. Convenience:\nUse APIs for simple, fast use-cases.\nDeploy locally for complex, customizable, or experimental projects that need advanced models.\nProject Plan for Quick Tangible Output:\nBuild an image captioning app using Llama 3.2 Vision Instruct.\nFollow structured steps: image preprocessing, model loading, generating captions, and UI integration.\nLearning Curve Strategy:\nStart small (single image captioning) and expand to advanced features (multi-image captioning).\nIteratively increase complexity to deepen understanding.\nNote on AWS Setup:\nThe AWS setup for running Meta’s Llama 3.2 Vision Instruct model involves creating an account, launching a GPU-supported EC2 instance, configuring storage, and managing dependencies. This provides full control but requires careful infrastructure management, like resizing storage and installing necessary packages.\nNext Step: Ready to kickstart your own image captioning project? Focus on quick wins and iterate for deeper insights!\nDate: October 17, 2024\nIntroduction: The challenges of selecting and configuring the right infrastructure for large AI models are part of the learning process. Today, it was all about recalibrating expectations, dealing with resource limitations, and adapting to practical constraints.\nCaption: “A Day of Reevaluation: Matching Model Ambitions to Infrastructure Realities”\nKey Takeaways:\nModel Compatibility Frustrations:\nAttempted to initialize the LLaMA-3.2-11B model, but hit memory limits on the g4dn.xlarge instance.\nLearned that selecting the right infrastructure is key, especially when handling large models that require significant GPU memory.\nServer Instance Challenges:\nFaced delays when stopping and restarting the AWS server, leading to confusion about limits and resource requirements.\nRealized the importance of matching the instance type (memory and GPU) to the model requirements.\nPivoting to a Smaller Model:\nDecided to switch to a smaller 1B version of the LLaMA model to align with current infrastructure capacity.\nThe journey included reapplying for model access, dealing with download and setup instructions, and adapting to unexpected issues with hosting platforms.\nRelying on Tools Can Be Both a Blessing and a Burden:\nRelied heavily on copilot tools like Cursor and GPT for syntax and setup guidance, which provided helpful suggestions but also added challenges in diagnosing complex issues.\nTaking a Step Back:\nDecided to take a step back and reflect on the journey so far, evaluating the reasons behind each technical decision and understanding their impact on the overall process. Reflection helps reinforce learning and ensures a clearer direction moving forward.\nNext Step: Now that the 1B model is downloaded and infrastructure requirements have been recalibrated, the focus will shift to getting the model up and running to start the actual project work. Keep adapting and moving forward!\nDate: October 18, 2024\nCaption: “The Art of Stepping Back: Reflecting on Complexity Before Moving Forward”\nKey Takeaways:\nDecision to Reflect:\nRealized the need to take a step back to evaluate the entire setup process and its complexities.\nReflection helps in understanding the impact of each decision and reinforces learning.\nBalancing Progress with Understanding:\nInstead of blindly pushing forward, it’s important to assess the reasons behind each technical change.\nTechniques like quantization, LoRA, and gradient checkpointing were effective, but understanding why they work is crucial for future success.\nPrioritizing Next Steps:\nComplete One Cycle of Fine-Tuning: Let the fine-tuning process finish to evaluate the model’s performance and observe how all the techniques come together.\nExperiment with Data Size: Reduce data size to speed up iteration and learn the fine-tuning process effectively before scaling up.\nAnalyze Training Speed: Training time has increased from 10s to 15s per iteration. Understanding the reasons for this change is essential for efficiency.\nEvaluate Early Stops: Implement early stopping or reduce the number of epochs to obtain a checkpoint model for quicker evaluation.\nUnderstand the Techniques: Take time to understand the individual impact of each technique (quantization, LoRA, etc.) on memory, speed, and accuracy.\nNext Step: Reflecting on the process so far has highlighted areas for optimization. The focus will now be on completing a full training cycle and understanding the impact of each optimization technique. Keep learning, adapting, and moving forward!\n",
  "wordCount" : "755",
  "inLanguage": "en",
  "datePublished": "2024-10-16T12:00:00Z",
  "dateModified": "2024-10-16T12:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://barath.xyz/posts/anotherpost/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "barath.xyz",
    "logo": {
      "@type": "ImageObject",
      "url": "https://barath.xyz/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://barath.xyz/" accesskey="h" title="barath.xyz (Alt + H)">barath.xyz</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Control or Convenience? Lessons from Implementing Vision-Language Models
    </h1>
    <div class="post-meta"><span title='2024-10-16 12:00:00 +0000 UTC'>October 16, 2024</span>

</div>
  </header> 
  <div class="post-content"><p>Date: October 16, 2024</p>
<p>Introduction: This blog captures the learning journey of implementing vision-language models, exploring trade-offs between convenience and control, and documenting the practical steps involved.</p>
<p>Caption: &ldquo;Control or Convenience? Lessons from Implementing Vision-Language Models&rdquo;</p>
<p>Key Takeaways:</p>
<p>Quick Wins for Learning:</p>
<p>Start with tangible projects to quickly gain experience with new models.</p>
<p>Image captioning with Llama 3.2 Vision Instruct is a great entry project to understand vision-language integration.</p>
<p>API vs. Local Model Deployment:</p>
<p>API Approach: Easy, fast setup with minimal control; ideal for prototyping.</p>
<p>Local Model Deployment: Requires infrastructure management but offers full control and cost efficiency for larger workloads.</p>
<p>Control vs. Convenience:</p>
<p>Use APIs for simple, fast use-cases.</p>
<p>Deploy locally for complex, customizable, or experimental projects that need advanced models.</p>
<p>Project Plan for Quick Tangible Output:</p>
<p>Build an image captioning app using Llama 3.2 Vision Instruct.</p>
<p>Follow structured steps: image preprocessing, model loading, generating captions, and UI integration.</p>
<p>Learning Curve Strategy:</p>
<p>Start small (single image captioning) and expand to advanced features (multi-image captioning).</p>
<p>Iteratively increase complexity to deepen understanding.</p>
<p>Note on AWS Setup:</p>
<p>The AWS setup for running Meta&rsquo;s Llama 3.2 Vision Instruct model involves creating an account, launching a GPU-supported EC2 instance, configuring storage, and managing dependencies. This provides full control but requires careful infrastructure management, like resizing storage and installing necessary packages.</p>
<p>Next Step: Ready to kickstart your own image captioning project? Focus on quick wins and iterate for deeper insights!</p>
<p>Date: October 17, 2024</p>
<p>Introduction: The challenges of selecting and configuring the right infrastructure for large AI models are part of the learning process. Today, it was all about recalibrating expectations, dealing with resource limitations, and adapting to practical constraints.</p>
<p>Caption: &ldquo;A Day of Reevaluation: Matching Model Ambitions to Infrastructure Realities&rdquo;</p>
<p>Key Takeaways:</p>
<p>Model Compatibility Frustrations:</p>
<p>Attempted to initialize the LLaMA-3.2-11B model, but hit memory limits on the g4dn.xlarge instance.</p>
<p>Learned that selecting the right infrastructure is key, especially when handling large models that require significant GPU memory.</p>
<p>Server Instance Challenges:</p>
<p>Faced delays when stopping and restarting the AWS server, leading to confusion about limits and resource requirements.</p>
<p>Realized the importance of matching the instance type (memory and GPU) to the model requirements.</p>
<p>Pivoting to a Smaller Model:</p>
<p>Decided to switch to a smaller 1B version of the LLaMA model to align with current infrastructure capacity.</p>
<p>The journey included reapplying for model access, dealing with download and setup instructions, and adapting to unexpected issues with hosting platforms.</p>
<p>Relying on Tools Can Be Both a Blessing and a Burden:</p>
<p>Relied heavily on copilot tools like Cursor and GPT for syntax and setup guidance, which provided helpful suggestions but also added challenges in diagnosing complex issues.</p>
<p>Taking a Step Back:</p>
<p>Decided to take a step back and reflect on the journey so far, evaluating the reasons behind each technical decision and understanding their impact on the overall process. Reflection helps reinforce learning and ensures a clearer direction moving forward.</p>
<p>Next Step: Now that the 1B model is downloaded and infrastructure requirements have been recalibrated, the focus will shift to getting the model up and running to start the actual project work. Keep adapting and moving forward!</p>
<p>Date: October 18, 2024</p>
<p>Caption: &ldquo;The Art of Stepping Back: Reflecting on Complexity Before Moving Forward&rdquo;</p>
<p>Key Takeaways:</p>
<p>Decision to Reflect:</p>
<p>Realized the need to take a step back to evaluate the entire setup process and its complexities.</p>
<p>Reflection helps in understanding the impact of each decision and reinforces learning.</p>
<p>Balancing Progress with Understanding:</p>
<p>Instead of blindly pushing forward, it&rsquo;s important to assess the reasons behind each technical change.</p>
<p>Techniques like quantization, LoRA, and gradient checkpointing were effective, but understanding why they work is crucial for future success.</p>
<p>Prioritizing Next Steps:</p>
<p>Complete One Cycle of Fine-Tuning: Let the fine-tuning process finish to evaluate the model’s performance and observe how all the techniques come together.</p>
<p>Experiment with Data Size: Reduce data size to speed up iteration and learn the fine-tuning process effectively before scaling up.</p>
<p>Analyze Training Speed: Training time has increased from 10s to 15s per iteration. Understanding the reasons for this change is essential for efficiency.</p>
<p>Evaluate Early Stops: Implement early stopping or reduce the number of epochs to obtain a checkpoint model for quicker evaluation.</p>
<p>Understand the Techniques: Take time to understand the individual impact of each technique (quantization, LoRA, etc.) on memory, speed, and accuracy.</p>
<p>Next Step: Reflecting on the process so far has highlighted areas for optimization. The focus will now be on completing a full training cycle and understanding the impact of each optimization technique. Keep learning, adapting, and moving forward!</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://barath.xyz/">barath.xyz</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
